{
    "uuid": "8b52ade7-c0f1-404c-9178-a8383b33876a",
    "model": "open_lm_1b_swiglutorch",
    "creation_date": "2025_08_18-05_01_51",
    "name": "/mnt/one/home/jmelko/dclm_exp/eval/mmlu_and_lowvar",
    "eval_metrics": {
        "perplexity": 2.46429927945137,
        "downstream_perpexity": {
            "mmlu": 1.6427316319278598,
            "hellaswag": 2.3751855450212136,
            "jeopardy_all": 1.5735578802913555,
            "bigbench_qa_wikidata": 3.8213260380995684,
            "arc_easy": 2.160106627076162,
            "arc_challenge": 2.3790747745045215,
            "copa": 2.491083083152771,
            "commonsense_qa": 1.634782091698424,
            "piqa": 2.5571450613787694,
            "openbook_qa": 4.080590865135193,
            "lambada_openai": 1.2664146226701194,
            "winograd_wsc": 2.474721442212115,
            "winogrande": 3.063713943102083,
            "bigbench_dyck_languages": 4.321376420974731,
            "agi_eval_lsat_ar": 1.6669184534446053,
            "bigbench_cs_algorithms": 3.1324296804991634,
            "bigbench_operators": 4.995121500605628,
            "bigbench_repeat_copy_logic": 1.3623223826289177,
            "squad": 3.2769204542111314,
            "coqa": 3.7912344939723943,
            "boolq": 2.9494093053566934,
            "bigbench_language_identification": 9.275646608146245
        },
        "icl": {
            "mmlu_fewshot": 0.2604746544047406,
            "hellaswag_zeroshot": 0.5812587141990662,
            "jeopardy": 0.2580664366483688,
            "bigbench_qa_wikidata": 0.5777766704559326,
            "arc_easy": 0.6464646458625793,
            "arc_challenge": 0.3464163839817047,
            "copa": 0.7300000190734863,
            "commonsense_qa": 0.3136773109436035,
            "piqa": 0.7388466000556946,
            "openbook_qa": 0.38999998569488525,
            "lambada_openai": 0.5922763347625732,
            "hellaswag": 0.5853415727615356,
            "winograd": 0.7582417726516724,
            "winogrande": 0.5808997750282288,
            "bigbench_dyck_languages": 0.19300000369548798,
            "agi_eval_lsat_ar": 0.269565224647522,
            "bigbench_cs_algorithms": 0.4325757622718811,
            "bigbench_operators": 0.1666666716337204,
            "bigbench_repeat_copy_logic": 0.03125,
            "squad": 0.3797540068626404,
            "coqa": 0.3011399209499359,
            "boolq": 0.5773700475692749,
            "bigbench_language_identification": 0.2565999925136566
        }
    },
    "missing tasks": "['mmlu_zeroshot', 'triviaqa_sm_sub', 'gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'bigbench_misconceptions', 'siqa', 'bigbench_novel_concepts', 'bigbench_strange_stories', 'bigbench_strategy_qa', 'bigbench_conlang_translation', 'bigbench_conceptual_combinations', 'bigbench_elementary_math_qa', 'bigbench_logical_deduction', 'simple_arithmetic_nospaces', 'simple_arithmetic_withspaces', 'math_qa', 'logi_qa', 'pubmed_qa_labeled', 'agi_eval_lsat_rc', 'agi_eval_lsat_lr', 'bigbench_understanding_fables', 'agi_eval_sat_en', 'winogender_mc_female', 'winogender_mc_male', 'enterprise_pii_classification', 'bbq', 'gpqa_main', 'gpqa_diamond']",
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.31661413113276166,
        "language understanding": 0.3902563370537487,
        "reading comprehension": 0.18956994748952097,
        "safety": NaN,
        "symbolic problem solving": 0.18208979368209838,
        "world knowledge": 0.30139680388726686
    },
    "aggregated_centered_results": 0.2867016250943012,
    "aggregated_results": 0.43337663072470395,
    "rw_small": 0.5770682642857233,
    "rw_small_centered": 0.26807808248620285,
    "95%_CI_above": 0.45002799332141874,
    "95%_CI_above_centered": 0.30466633150999806,
    "99%_CI_above": 0.45002799332141874,
    "99%_CI_above_centered": 0.30466633150999806,
    "low_variance_datasets": 0.44123581146652047,
    "low_variance_datasets_centered": 0.2990986896043609,
    "Core": 0.2990986896043609,
    "Extended": "N/A due to missing tasks: ['mmlu_zeroshot', 'triviaqa_sm_sub', 'gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'bigbench_misconceptions', 'siqa', 'bigbench_novel_concepts', 'bigbench_strange_stories', 'bigbench_strategy_qa', 'bigbench_conlang_translation', 'bigbench_conceptual_combinations', 'bigbench_elementary_math_qa', 'bigbench_logical_deduction', 'simple_arithmetic_nospaces', 'simple_arithmetic_withspaces', 'math_qa', 'logi_qa', 'pubmed_qa_labeled', 'agi_eval_lsat_rc', 'agi_eval_lsat_lr', 'bigbench_understanding_fables', 'agi_eval_sat_en', 'winogender_mc_female', 'winogender_mc_male', 'enterprise_pii_classification', 'bbq', 'gpqa_main', 'gpqa_diamond']",
    "model_uuid": "12fdfae2-eaf2-4ee8-ba47-f303df2df53e"
}