{
    "uuid": "0ba3dbf1-c43b-4694-a80d-c490ec871583",
    "model": "open_lm_1b_swiglutorch",
    "creation_date": "2025_08_18-07_59_08",
    "name": "/mnt/one/home/jmelko/dclm_exp/eval/mmlu_and_lowvar",
    "eval_metrics": {
        "perplexity": 2.496723719437917,
        "downstream_perpexity": {
            "mmlu": 1.562834508193896,
            "hellaswag": 2.395242810320555,
            "jeopardy_all": 1.743860688421261,
            "bigbench_qa_wikidata": 3.9259954609684833,
            "arc_easy": 2.3489177746303156,
            "arc_challenge": 2.5188078456669536,
            "copa": 2.5643697583675387,
            "commonsense_qa": 1.479072185054751,
            "piqa": 2.5859741174876367,
            "openbook_qa": 4.140048563480377,
            "lambada_openai": 1.391852202393113,
            "winograd_wsc": 2.531262682471083,
            "winogrande": 3.084519378011774,
            "bigbench_dyck_languages": 3.4682761180400847,
            "agi_eval_lsat_ar": 1.6152056559272434,
            "bigbench_cs_algorithms": 4.380268643660979,
            "bigbench_operators": 5.038637745948065,
            "bigbench_repeat_copy_logic": 1.4658849723637104,
            "squad": 1.8824100732239442,
            "coqa": 1.6168455148749703,
            "boolq": 3.0112109528404494,
            "bigbench_language_identification": 1.8745494524009896
        },
        "icl": {
            "mmlu_fewshot": 0.26615571844996067,
            "hellaswag_zeroshot": 0.5310695171356201,
            "jeopardy": 0.1957515999674797,
            "bigbench_qa_wikidata": 0.5588799715042114,
            "arc_easy": 0.6262626051902771,
            "arc_challenge": 0.3302047848701477,
            "copa": 0.6899999976158142,
            "commonsense_qa": 0.20720720291137695,
            "piqa": 0.7328618168830872,
            "openbook_qa": 0.3799999952316284,
            "lambada_openai": 0.5656898617744446,
            "hellaswag": 0.5327624082565308,
            "winograd": 0.7142857313156128,
            "winogrande": 0.5595895648002625,
            "bigbench_dyck_languages": 0.3109999895095825,
            "agi_eval_lsat_ar": 0.2347826063632965,
            "bigbench_cs_algorithms": 0.37272727489471436,
            "bigbench_operators": 0.17619048058986664,
            "bigbench_repeat_copy_logic": 0.03125,
            "squad": 0.32090821862220764,
            "coqa": 0.26681697368621826,
            "boolq": 0.5587155818939209,
            "bigbench_language_identification": 0.25029999017715454
        }
    },
    "missing tasks": "['mmlu_zeroshot', 'triviaqa_sm_sub', 'gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'bigbench_misconceptions', 'siqa', 'bigbench_novel_concepts', 'bigbench_strange_stories', 'bigbench_strategy_qa', 'bigbench_conlang_translation', 'bigbench_conceptual_combinations', 'bigbench_elementary_math_qa', 'bigbench_logical_deduction', 'simple_arithmetic_nospaces', 'simple_arithmetic_withspaces', 'math_qa', 'logi_qa', 'pubmed_qa_labeled', 'agi_eval_lsat_rc', 'agi_eval_lsat_lr', 'bigbench_understanding_fables', 'agi_eval_sat_en', 'winogender_mc_female', 'winogender_mc_male', 'enterprise_pii_classification', 'bbq', 'gpqa_main', 'gpqa_diamond']",
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.25701648990313214,
        "language understanding": 0.3400773114125546,
        "reading comprehension": 0.14215013593958137,
        "safety": NaN,
        "symbolic problem solving": 0.18692920058965684,
        "world knowledge": 0.276959143230441
    },
    "aggregated_centered_results": 0.25280095456552565,
    "aggregated_results": 0.4092787778975398,
    "rw_small": 0.5493070085843405,
    "rw_small_centered": 0.2202144805450886,
    "95%_CI_above": 0.42366458014363334,
    "95%_CI_above_centered": 0.26750635633940684,
    "99%_CI_above": 0.42366458014363334,
    "99%_CI_above_centered": 0.26750635633940684,
    "low_variance_datasets": 0.4157843715087934,
    "low_variance_datasets_centered": 0.26331277259426406,
    "Core": 0.26331277259426406,
    "Extended": "N/A due to missing tasks: ['mmlu_zeroshot', 'triviaqa_sm_sub', 'gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'bigbench_misconceptions', 'siqa', 'bigbench_novel_concepts', 'bigbench_strange_stories', 'bigbench_strategy_qa', 'bigbench_conlang_translation', 'bigbench_conceptual_combinations', 'bigbench_elementary_math_qa', 'bigbench_logical_deduction', 'simple_arithmetic_nospaces', 'simple_arithmetic_withspaces', 'math_qa', 'logi_qa', 'pubmed_qa_labeled', 'agi_eval_lsat_rc', 'agi_eval_lsat_lr', 'bigbench_understanding_fables', 'agi_eval_sat_en', 'winogender_mc_female', 'winogender_mc_male', 'enterprise_pii_classification', 'bbq', 'gpqa_main', 'gpqa_diamond']",
    "model_uuid": "7544d0f7-a903-4bea-9de4-200f48ea1494"
}