{
    "uuid": "a0c03238-ada8-4529-8fe3-fe0b005ff939",
    "model": "d=1024_l=24_h=8",
    "creation_date": "2025_08_18-04_04_15",
    "name": "/mnt/one/home/jmelko/dclm_exp/eval/mmlu_and_lowvar",
    "eval_metrics": {
        "perplexity": 2.8343174775441486,
        "downstream_perpexity": {
            "mmlu": 1.633060118671904,
            "hellaswag": 2.729452142960473,
            "jeopardy_all": 2.746823354211122,
            "bigbench_qa_wikidata": 4.92959394516435,
            "arc_easy": 2.8130957715037694,
            "arc_challenge": 2.937150364493754,
            "copa": 2.8542114305496216,
            "commonsense_qa": 1.603455284401396,
            "piqa": 2.947017986704398,
            "openbook_qa": 4.540032869338989,
            "lambada_openai": 1.9368711002799994,
            "winograd_wsc": 2.805182001311264,
            "winogrande": 3.2878466933942163,
            "bigbench_dyck_languages": 4.579194890975952,
            "agi_eval_lsat_ar": 1.620863135482954,
            "bigbench_cs_algorithms": 4.953705270723863,
            "bigbench_operators": 5.5449084940410795,
            "bigbench_repeat_copy_logic": 1.8700235821306705,
            "squad": 4.046608331954581,
            "coqa": 4.634088137447573,
            "boolq": 2.703080568838557,
            "bigbench_language_identification": 10.971532264979608
        },
        "icl": {
            "mmlu_fewshot": 0.2469087622144766,
            "hellaswag_zeroshot": 0.37741485238075256,
            "jeopardy": 0.036452943366020917,
            "bigbench_qa_wikidata": 0.4325082302093506,
            "arc_easy": 0.5176767706871033,
            "arc_challenge": 0.2653583586215973,
            "copa": 0.5699999928474426,
            "commonsense_qa": 0.1941031962633133,
            "piqa": 0.6648530960083008,
            "openbook_qa": 0.3179999887943268,
            "lambada_openai": 0.4205317199230194,
            "hellaswag": 0.3771161139011383,
            "winograd": 0.6483516693115234,
            "winogrande": 0.5169692039489746,
            "bigbench_dyck_languages": 0.1770000010728836,
            "agi_eval_lsat_ar": 0.30000001192092896,
            "bigbench_cs_algorithms": 0.4469696879386902,
            "bigbench_operators": 0.15238095819950104,
            "bigbench_repeat_copy_logic": 0.0,
            "squad": 0.125827819108963,
            "coqa": 0.1503194272518158,
            "boolq": 0.5461773872375488,
            "bigbench_language_identification": 0.2515999972820282
        }
    },
    "missing tasks": "['mmlu_zeroshot', 'triviaqa_sm_sub', 'gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'bigbench_misconceptions', 'siqa', 'bigbench_novel_concepts', 'bigbench_strange_stories', 'bigbench_strategy_qa', 'bigbench_conlang_translation', 'bigbench_conceptual_combinations', 'bigbench_elementary_math_qa', 'bigbench_logical_deduction', 'simple_arithmetic_nospaces', 'simple_arithmetic_withspaces', 'math_qa', 'logi_qa', 'pubmed_qa_labeled', 'agi_eval_lsat_rc', 'agi_eval_lsat_lr', 'bigbench_understanding_fables', 'agi_eval_sat_en', 'winogender_mc_female', 'winogender_mc_male', 'enterprise_pii_classification', 'bbq', 'gpqa_main', 'gpqa_diamond']",
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.13825045619159937,
        "language understanding": 0.21120429215498535,
        "reading comprehension": 0.02729240425846034,
        "safety": NaN,
        "symbolic problem solving": 0.18027013242244722,
        "world knowledge": 0.16844393912125488
    },
    "aggregated_centered_results": 0.15850761513870437,
    "aggregated_results": 0.33637044297781304,
    "rw_small": 0.4655994276205699,
    "rw_small_centered": 0.08782250037667345,
    "95%_CI_above": 0.34466721090443786,
    "95%_CI_above_centered": 0.16538662541877094,
    "99%_CI_above": 0.34466721090443786,
    "99%_CI_above_centered": 0.16538662541877097,
    "low_variance_datasets": 0.3404368830125101,
    "low_variance_datasets_centered": 0.16589985448049538,
    "Core": 0.16589985448049538,
    "Extended": "N/A due to missing tasks: ['mmlu_zeroshot', 'triviaqa_sm_sub', 'gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'bigbench_misconceptions', 'siqa', 'bigbench_novel_concepts', 'bigbench_strange_stories', 'bigbench_strategy_qa', 'bigbench_conlang_translation', 'bigbench_conceptual_combinations', 'bigbench_elementary_math_qa', 'bigbench_logical_deduction', 'simple_arithmetic_nospaces', 'simple_arithmetic_withspaces', 'math_qa', 'logi_qa', 'pubmed_qa_labeled', 'agi_eval_lsat_rc', 'agi_eval_lsat_lr', 'bigbench_understanding_fables', 'agi_eval_sat_en', 'winogender_mc_female', 'winogender_mc_male', 'enterprise_pii_classification', 'bbq', 'gpqa_main', 'gpqa_diamond']",
    "model_uuid": "98c7871a-68b5-43fa-a0b6-1e0184fac70f"
}