{
    "uuid": "3c325c46-bd3f-46ef-9216-f172617fb0db",
    "model": "d=1024_l=24_h=8",
    "creation_date": "2025_08_18-07_01_46",
    "name": "/mnt/one/home/jmelko/dclm_exp/eval/mmlu_and_lowvar",
    "eval_metrics": {
        "perplexity": 2.8422511796156567,
        "downstream_perpexity": {
            "mmlu": 1.5858275034610676,
            "hellaswag": 2.719634963610903,
            "jeopardy_all": 2.7618462755593365,
            "bigbench_qa_wikidata": 5.238910786555652,
            "arc_easy": 2.811461178140608,
            "arc_challenge": 2.91631569024239,
            "copa": 2.8599189710617066,
            "commonsense_qa": 1.51871779988185,
            "piqa": 2.934157485411917,
            "openbook_qa": 4.533896925926208,
            "lambada_openai": 1.9533447565533641,
            "winograd_wsc": 2.8106991157426937,
            "winogrande": 3.290571352771256,
            "bigbench_dyck_languages": 4.25961944937706,
            "agi_eval_lsat_ar": 1.5567450906919396,
            "bigbench_cs_algorithms": 5.134238642996007,
            "bigbench_operators": 5.947200028101603,
            "bigbench_repeat_copy_logic": 1.819352276623249,
            "squad": 3.5973933876068274,
            "coqa": 4.2417680974384755,
            "boolq": 3.355452932276128,
            "bigbench_language_identification": 10.335661594522884
        },
        "icl": {
            "mmlu_fewshot": 0.23894076258466954,
            "hellaswag_zeroshot": 0.3760206997394562,
            "jeopardy": 0.0226006580516696,
            "bigbench_qa_wikidata": 0.4077555239200592,
            "arc_easy": 0.5172559022903442,
            "arc_challenge": 0.27901023626327515,
            "copa": 0.6200000047683716,
            "commonsense_qa": 0.23587223887443542,
            "piqa": 0.6702938079833984,
            "openbook_qa": 0.3160000145435333,
            "lambada_openai": 0.4123811423778534,
            "hellaswag": 0.3772156834602356,
            "winograd": 0.6263736486434937,
            "winogrande": 0.49723756313323975,
            "bigbench_dyck_languages": 0.21799999475479126,
            "agi_eval_lsat_ar": 0.239130437374115,
            "bigbench_cs_algorithms": 0.4522727131843567,
            "bigbench_operators": 0.14761905372142792,
            "bigbench_repeat_copy_logic": 0.03125,
            "squad": 0.1416272521018982,
            "coqa": 0.16297131776809692,
            "boolq": 0.5902140736579895,
            "bigbench_language_identification": 0.25209999084472656
        }
    },
    "missing tasks": "['mmlu_zeroshot', 'triviaqa_sm_sub', 'gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'bigbench_misconceptions', 'siqa', 'bigbench_novel_concepts', 'bigbench_strange_stories', 'bigbench_strategy_qa', 'bigbench_conlang_translation', 'bigbench_conceptual_combinations', 'bigbench_elementary_math_qa', 'bigbench_logical_deduction', 'simple_arithmetic_nospaces', 'simple_arithmetic_withspaces', 'math_qa', 'logi_qa', 'pubmed_qa_labeled', 'agi_eval_lsat_rc', 'agi_eval_lsat_lr', 'bigbench_understanding_fables', 'agi_eval_sat_en', 'winogender_mc_female', 'winogender_mc_male', 'enterprise_pii_classification', 'bbq', 'gpqa_main', 'gpqa_diamond']",
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.17835698587199053,
        "language understanding": 0.19574663159411865,
        "reading comprehension": 0.0754048510601646,
        "safety": NaN,
        "symbolic problem solving": 0.17961096167564392,
        "world knowledge": 0.1621264100312228
    },
    "aggregated_centered_results": 0.16620909325076094,
    "aggregated_results": 0.3405279443496277,
    "rw_small": 0.4799564133087794,
    "rw_small_centered": 0.119856706139637,
    "95%_CI_above": 0.3495762841243829,
    "95%_CI_above_centered": 0.17430128961686378,
    "99%_CI_above": 0.3495762841243829,
    "99%_CI_above_centered": 0.17430128961686378,
    "low_variance_datasets": 0.34514554352076215,
    "low_variance_datasets_centered": 0.17443430884793676,
    "Core": 0.17443430884793676,
    "Extended": "N/A due to missing tasks: ['mmlu_zeroshot', 'triviaqa_sm_sub', 'gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'bigbench_misconceptions', 'siqa', 'bigbench_novel_concepts', 'bigbench_strange_stories', 'bigbench_strategy_qa', 'bigbench_conlang_translation', 'bigbench_conceptual_combinations', 'bigbench_elementary_math_qa', 'bigbench_logical_deduction', 'simple_arithmetic_nospaces', 'simple_arithmetic_withspaces', 'math_qa', 'logi_qa', 'pubmed_qa_labeled', 'agi_eval_lsat_rc', 'agi_eval_lsat_lr', 'bigbench_understanding_fables', 'agi_eval_sat_en', 'winogender_mc_female', 'winogender_mc_male', 'enterprise_pii_classification', 'bbq', 'gpqa_main', 'gpqa_diamond']",
    "model_uuid": "82e032f7-c67c-49f0-9caa-e7414875c6ab"
}