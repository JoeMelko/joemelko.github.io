{
    "uuid": "ae94fcbb-5621-48f2-b55d-dd7046b78d76",
    "model": "open_lm_1b_swiglutorch",
    "creation_date": "2025_08_18-05_59_17",
    "name": "/mnt/one/home/jmelko/dclm_exp/eval/mmlu_and_lowvar",
    "eval_metrics": {
        "perplexity": 2.4436374644438428,
        "downstream_perpexity": {
            "mmlu": 1.6918626094013856,
            "hellaswag": 2.3740404423810744,
            "jeopardy_all": 1.5918055071082384,
            "bigbench_qa_wikidata": 3.6649257146633434,
            "arc_easy": 2.1419644651790257,
            "arc_challenge": 2.361204188634918,
            "copa": 2.4402479577064513,
            "commonsense_qa": 1.7439954199818293,
            "piqa": 2.558371398472293,
            "openbook_qa": 4.047080261707306,
            "lambada_openai": 1.2859908492732182,
            "winograd_wsc": 2.4592853596795607,
            "winogrande": 3.0762649681404435,
            "bigbench_dyck_languages": 3.417888235092163,
            "agi_eval_lsat_ar": 1.6339852452278136,
            "bigbench_cs_algorithms": 3.6446860750516255,
            "bigbench_operators": 4.513445237137023,
            "bigbench_repeat_copy_logic": 1.1940047442913055,
            "squad": 3.2492724333716207,
            "coqa": 3.6284067795925625,
            "boolq": 2.995065753758865,
            "bigbench_language_identification": 9.226154923367487
        },
        "icl": {
            "mmlu_fewshot": 0.25154424210389453,
            "hellaswag_zeroshot": 0.5802628993988037,
            "jeopardy": 0.2620516955852509,
            "bigbench_qa_wikidata": 0.5980020761489868,
            "arc_easy": 0.6489899158477783,
            "arc_challenge": 0.3686006963253021,
            "copa": 0.699999988079071,
            "commonsense_qa": 0.20147420465946198,
            "piqa": 0.737758457660675,
            "openbook_qa": 0.37400001287460327,
            "lambada_openai": 0.5969338417053223,
            "hellaswag": 0.5922126770019531,
            "winograd": 0.761904776096344,
            "winogrande": 0.5619573593139648,
            "bigbench_dyck_languages": 0.2709999978542328,
            "agi_eval_lsat_ar": 0.208695650100708,
            "bigbench_cs_algorithms": 0.4363636374473572,
            "bigbench_operators": 0.19523809850215912,
            "bigbench_repeat_copy_logic": 0.0625,
            "squad": 0.38022705912590027,
            "coqa": 0.3012651801109314,
            "boolq": 0.5810397267341614,
            "bigbench_language_identification": 0.2563999891281128
        }
    },
    "missing tasks": "['mmlu_zeroshot', 'triviaqa_sm_sub', 'gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'bigbench_misconceptions', 'siqa', 'bigbench_novel_concepts', 'bigbench_strange_stories', 'bigbench_strategy_qa', 'bigbench_conlang_translation', 'bigbench_conceptual_combinations', 'bigbench_elementary_math_qa', 'bigbench_logical_deduction', 'simple_arithmetic_nospaces', 'simple_arithmetic_withspaces', 'math_qa', 'logi_qa', 'pubmed_qa_labeled', 'agi_eval_lsat_rc', 'agi_eval_lsat_lr', 'bigbench_understanding_fables', 'agi_eval_sat_en', 'winogender_mc_female', 'winogender_mc_male', 'enterprise_pii_classification', 'bbq', 'gpqa_main', 'gpqa_diamond']",
    "aggregated_task_categories_centered": {
        "commonsense reasoning": 0.26067324945082265,
        "language understanding": 0.3872083997088209,
        "reading comprehension": 0.19298840144224336,
        "safety": NaN,
        "symbolic problem solving": 0.19519425928592682,
        "world knowledge": 0.3104467154873742
    },
    "aggregated_centered_results": 0.2814397162717153,
    "aggregated_results": 0.4316705296436946,
    "rw_small": 0.5734750976165136,
    "rw_small_centered": 0.2586431681064137,
    "95%_CI_above": 0.4485941881225223,
    "95%_CI_above_centered": 0.2994807762342039,
    "99%_CI_above": 0.4485941881225223,
    "99%_CI_above_centered": 0.2994807762342039,
    "low_variance_datasets": 0.43985808816823097,
    "low_variance_datasets_centered": 0.2941388402171633,
    "Core": 0.2941388402171633,
    "Extended": "N/A due to missing tasks: ['mmlu_zeroshot', 'triviaqa_sm_sub', 'gsm8k_cot', 'agi_eval_sat_math_cot', 'aqua_cot', 'svamp_cot', 'bigbench_misconceptions', 'siqa', 'bigbench_novel_concepts', 'bigbench_strange_stories', 'bigbench_strategy_qa', 'bigbench_conlang_translation', 'bigbench_conceptual_combinations', 'bigbench_elementary_math_qa', 'bigbench_logical_deduction', 'simple_arithmetic_nospaces', 'simple_arithmetic_withspaces', 'math_qa', 'logi_qa', 'pubmed_qa_labeled', 'agi_eval_lsat_rc', 'agi_eval_lsat_lr', 'bigbench_understanding_fables', 'agi_eval_sat_en', 'winogender_mc_female', 'winogender_mc_male', 'enterprise_pii_classification', 'bbq', 'gpqa_main', 'gpqa_diamond']",
    "model_uuid": "09dc9b35-ce97-47d1-869a-e96925f0773f"
}